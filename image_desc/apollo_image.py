import json
import boto3
import time
import logging
import base64
import imghdr
from io import BytesIO
from PIL import Image
import asyncio
from openai import AsyncClient
from tqdm.asyncio import tqdm_asyncio
import re
import io


# =============================
# é…ç½®åŒºåŸŸ
# =============================
S3_CONFIG = {
    "aws_access_key_id": "C6360C346C985CF1EBE5",
    "aws_secret_access_key": "9DH4qbFGepkU8P96pP4kG7d0V14AAAGWbJhc8VJ1",
    "endpoint_url": "http://d-ceph-ssd-inside.pjlab.org.cn",
}

# BUCKET_NAME = 'heta'
# INPUT_PREFIX = 'test/element/30/ecnu/en_web_brookings/brookings_html/'
# INPUT_JSONL = INPUT_PREFIX + 'jsonl/'
# INPUT_IMAGE = INPUT_PREFIX + 'image/'
# OUTPUT_IMAGE_DESC = INPUT_PREFIX + 'image_desc/'

BUCKET_NAME = 'heta'
INPUT_PREFIX = 'element/ecnu/Apollo/Apollo_pdf/'
INPUT_JSONL = INPUT_PREFIX + 'jsonl/'
INPUT_IMAGE = INPUT_PREFIX + 'imgs/'
OUTPUT_IMAGE_DESC = INPUT_PREFIX + 'image_desc/'



# æ‰¹æ¬¡å¤§å° 
BATCH_SIZE = 1

# æ—¥å¿—è®¾ç½®
logging.getLogger("httpx").setLevel(logging.WARNING)

# === å¼‚æ­¥å®¢æˆ·ç«¯ ===
client = AsyncClient(
    api_key="EMPTY",
    base_url="http://10.140.37.15:8007/v1/"
)

# æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
MAX_CONCURRENT_REQUESTS = 20
semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

# =============================
# å·¥å…·å‡½æ•°åŒº
# =============================
def is_valid_image(image_data) -> tuple[bool, str]:
    try:
        if not image_data:
            return False, "å›¾ç‰‡æ•°æ®ä¸ºç©º"
        image = Image.open(BytesIO(image_data))
        image.verify()
        return True, ""
    except Exception as e:
        return False, str(e)

def get_image_mime(image_data):
    img_type = imghdr.what(None, image_data)
    mime_map = {
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif',
        'bmp': 'image/bmp',
        'webp': 'image/webp'
    }
    return mime_map.get(img_type)

# === å¼‚æ­¥å›¾ç‰‡æè¿°ç”Ÿæˆå‡½æ•° ===
async def get_image_desc_async(image_data: bytes, ref_text: str, caption: str) -> str:
    valid, error_msg = is_valid_image(image_data)
    if not valid:
        # logging.warning(f"è·³è¿‡æ— æ•ˆå›¾ç‰‡: {error_msg}")
        return ""

    base64_str = base64.b64encode(image_data).decode("utf-8")
    mime_type = get_image_mime(image_data) or 'image/jpeg'

    caption_is_available = f"è¿™å¼ å›¾ç‰‡çš„captionæ˜¯{caption}" if caption else ""
    text_prompt = (
        "è¯·ç»™è¿™å¼ å›¾ç‰‡æä¾›è¯´æ˜ï¼Œè¯†åˆ«å›¾ä¸­å…³é”®æ ‡è¯†æ€§å…ƒç´ ï¼Œå¹¶æ¨æµ‹å›¾ç‰‡æ ‡é¢˜ï¼›\n"
        f"{caption_is_available}\n"
        f"å¯å‚è€ƒæ–‡å­—å†…å®¹ï¼š{ref_text}ï¼Œä½†è¦ä»”ç»†ç”„åˆ«å‡ºä¸å›¾ç‰‡ç›¸å…³çš„å†…å®¹\n"
        "è¦æ±‚è¯­è¨€ç®€æ´å‡ç»ƒï¼Œä¸è¦æè¿°ç”»é¢å¸ƒå±€ï¼›è¾“å‡ºæ ¼å¼ï¼šæ ‡é¢˜--å›¾ç‰‡è¯´æ˜ã€‚"
    )

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_str}"}},
                {"type": "text", "text": text_prompt}
            ]
        }
    ]

    try:
        async with semaphore:  # æ§åˆ¶å¹¶å‘
            response = await client.chat.completions.create(
                model="Qwen2.5-VL-72B-Instruct",
                messages=messages,
                max_tokens=1024,
                temperature=0.1,
                stream=False
            )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"è°ƒç”¨æ¨¡å‹å¤±è´¥: {e}")
        return ""

# æ—¶é—´æ ¼å¼åŒ–
def format_time(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours}h {minutes}m {secs}s"

# =============================
# æ‰¹æ¬¡å¤„ç†å‡½æ•°
# =============================
async def process_batch(s3_client, batch_file_keys, output_keys_set):
    """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ–‡ä»¶ï¼Œè¿”å›æœ‰æ•ˆå›¾ç‰‡æ•°é‡"""
    print(f"å¼€å§‹å¤„ç†æ‰¹æ¬¡ï¼ŒåŒ…å« {len(batch_file_keys)} ä¸ªæ–‡ä»¶")
    
    # å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœå®¹å™¨
    tasks = []
    task_metadata = []
    # key: (input_file_key, line_index), value: {"meta": ..., "processed_items": [...]}
    file_line_results = {}

    batch_start_time = time.time()
    valid_image_count = 0  # æœ‰æ•ˆå›¾ç‰‡è®¡æ•°å™¨

    # éå†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¶é›†ä»»åŠ¡
    for file_key in batch_file_keys:
        output_key = file_key.replace(INPUT_JSONL, OUTPUT_IMAGE_DESC)
        # if output_key in output_keys_set:
        #     print(f"è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_key} -> {output_key}")
        #     continue

        print(f"è¯»å–æ–‡ä»¶: {file_key}")
        try:
            response = s3_client.get_object(Bucket=BUCKET_NAME, Key=file_key)
            content = response['Body'].read().decode('utf-8')
            lines = [line.strip() for line in content.split('\n') if line.strip()]
        except Exception as e:
            logging.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_key}: {e}")
            continue

        # å¤„ç†æ¯ä¸ªJSONè¡Œ
        for line_index, json_line in enumerate(lines):
            if line_index % 1000 == 0:
                print(f"  å·²è¯»å– {line_index} è¡Œ...")
            try:
                data = json.loads(json_line)
            except Exception as e:
                logging.error(f"æ— æ³•è§£ææ–‡ä»¶ {file_key} ç¬¬ {line_index} è¡Œ: {e}")
                continue

            if "json_content" not in data:
                continue

            # ä¿å­˜åŸå§‹metaä¿¡æ¯
            original_meta = data.get("meta", {})
            
            # æå–æ‰€æœ‰ page_x çš„é”®ï¼Œå¹¶æŒ‰æ•°å­—æ’åº
            page_keys = sorted(
                (k for k in data["json_content"].keys() if k.startswith("page_")),
                key=lambda k: int(re.search(r'\d+$', k).group())
            )

            page_texts = {}
            images_in_line = []

            for page_key in page_keys:
                page_list = data["json_content"][page_key]
                if not page_list:
                    continue  # è·³è¿‡ç©ºåˆ—è¡¨
                if page_list[-1].get("type") == "merge_text":
                    page_texts[page_key] = page_list[-1]["text"]
                images_in_line.extend(item for item in page_list if item.get("type") == "image")

            # åˆå§‹åŒ–è¯¥è¡Œçš„ç»“æœå®¹å™¨
            file_line_key = (file_key, line_index)
            file_line_results[file_line_key] = {
                "meta": original_meta,
                "original_json_content": data.get("json_content", {}),  # ä¿å­˜åŸå§‹ç»“æ„
                "processed_items": []
            }

            for image_item in images_in_line:
                cnt = int(image_item["id"].split("_")[1])
                text = " ".join([page_texts.get(f"page_{cnt + i}", "") for i in [-1, 0, 1]])
                if "meta" in data and "description" in data["meta"]:
                    text = data["meta"]["description"] + " " + text
                image_item["desc"] = text

                # æ„å»ºä»»åŠ¡
                if "web_url" in image_item:
                    image_key = INPUT_IMAGE + image_item["web_url"]
                elif "url" in image_item:
                    image_key = INPUT_IMAGE + image_item["url"]
                try:
                    response = s3_client.get_object(Bucket=BUCKET_NAME, Key=image_key)
                    image_content = response['Body'].read()
                except Exception as e:
                    logging.error(f"æ— æ³•è¯»å–å›¾ç‰‡ {image_key}: {e}")
                    image_item["desc"] = ""
                    file_line_results[file_line_key]["processed_items"].append(image_item)
                    continue

                ref_text = image_item["desc"]
                caption = image_item.get("caption", "")

                task = get_image_desc_async(image_content, ref_text, caption)
                tasks.append(task)
                task_metadata.append({
                    "file_line_key": file_line_key,
                    "image_item": image_item
                })

    if not tasks:
        print("è¯¥æ‰¹æ¬¡æ²¡æœ‰éœ€è¦å¤„ç†çš„ä»»åŠ¡")
        return 0

    print(f"è¯¥æ‰¹æ¬¡å…±æ”¶é›†åˆ° {len(tasks)} ä¸ªå›¾ç‰‡ä»»åŠ¡ï¼Œå¼€å§‹å¹¶è¡Œå¤„ç†...")

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await tqdm_asyncio.gather(*tasks, desc="å¤„ç†å›¾ç‰‡", total=len(tasks))

    # å°†ç»“æœå›å¡«åˆ°å¯¹åº”çš„è¡Œç»“æœä¸­
    for meta, result in zip(task_metadata, results):
        file_line_key = meta["file_line_key"]
        image_item = meta["image_item"]

        if isinstance(result, Exception):
            image_item["desc"] = ""
        else:
            image_item["desc"] = result

        # å¦‚æœæè¿°ä¸ä¸ºç©ºï¼Œè®¡æ•°å™¨åŠ ä¸€
        if image_item["desc"].strip():
            valid_image_count += 1

        file_line_results[file_line_key]["processed_items"].append(image_item)

    # å†™å›æ¯ä¸ªæ–‡ä»¶çš„ç»“æœ
    print("å¼€å§‹å†™å›ç»“æœ...")
    
    # æŒ‰æ–‡ä»¶åˆ†ç»„å¤„ç†ç»“æœ
    file_outputs = {}
    for (file_key, line_index), result_data in file_line_results.items():
        if file_key not in file_outputs:
            file_outputs[file_key] = {}
        file_outputs[file_key][line_index] = result_data

    # ä¸ºæ¯ä¸ªæ–‡ä»¶å†™å…¥ç»“æœ
    for file_key, line_results in file_outputs.items():
        output_key = file_key.replace(INPUT_JSONL, OUTPUT_IMAGE_DESC)
        output_stream = BytesIO()

        # æŒ‰è¡Œé¡ºåºå†™å…¥ç»“æœ
        for line_index in sorted(line_results.keys()):
            result_data = line_results[line_index]
            
            # é‡å»ºjson_contentï¼ŒåªåŒ…å«å¤„ç†è¿‡çš„å›¾ç‰‡é¡¹
            new_json_content = {}
            
            # æŒ‰é¡µé¢åˆ†ç»„å¤„ç†è¿‡çš„å›¾ç‰‡é¡¹
            for image_item in result_data["processed_items"]:
                page_id = image_item["id"].split("_")[1]
                page_key = f"page_{page_id}"
                if page_key not in new_json_content:
                    new_json_content[page_key] = []
                new_json_content[page_key].append(image_item)

            # æ„å»ºè¾“å‡ºæ•°æ®ï¼Œä¿æŒåŸæœ‰è¡Œç»“æ„
            # åªæœ‰å½“new_json_contentä¸ä¸ºç©ºæ—¶æ‰å†™å…¥
            if new_json_content:
                # æ„å»ºè¾“å‡ºæ•°æ®ï¼Œä¿æŒåŸæœ‰è¡Œç»“æ„
                output_data = {
                    "meta": result_data["meta"],
                    "json_content": new_json_content
                }

                json_line = json.dumps(output_data, ensure_ascii=False)
                output_stream.write(json_line.encode('utf-8'))
                output_stream.write(b'\n')

        # ä¸Šä¼ ç»“æœ
        if output_stream.tell() > 0:  # åªæœ‰å½“æœ‰å†…å®¹æ—¶æ‰ä¸Šä¼ 
            output_stream.seek(0)
            s3_client.upload_fileobj(
                Key=output_key,
                Fileobj=output_stream,
                Bucket=BUCKET_NAME,
                ExtraArgs={'ContentType': 'application/json'}
            )
            print(f"ç»“æœå·²ä¸Šä¼ : s3://{BUCKET_NAME}/{output_key}")

    batch_time = time.time() - batch_start_time
    print(f"æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œè€—æ—¶: {format_time(batch_time)}")
    print(f"æœ¬æ‰¹æ¬¡æœ‰æ•ˆå›¾ç‰‡æ•°é‡: {valid_image_count}")
    return valid_image_count  # è¿”å›æœ‰æ•ˆå›¾ç‰‡æ•°é‡

# =============================
# ä¸»ç¨‹åºå…¥å£
# =============================
async def main():
    s3_client = boto3.client("s3", **S3_CONFIG)

    # åˆ—å‡ºæ‰€æœ‰è¾“å…¥ JSONL æ–‡ä»¶
    paginator = s3_client.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=INPUT_JSONL)
    file_keys = [
        obj['Key'] for page in pages for obj in page.get('Contents', [])
        if obj['Key'].lower().endswith('.jsonl')
    ]
    print(f"æ‰¾åˆ° {len(file_keys)} ä¸ªå¾…å¤„ç†çš„ JSONL æ–‡ä»¶")

    # åˆ—å‡ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    output_keys_set = set()
    output_paginator = s3_client.get_paginator('list_objects_v2')
    output_pages = output_paginator.paginate(Bucket=BUCKET_NAME, Prefix=OUTPUT_IMAGE_DESC)
    for page in output_pages:
        for obj in page.get('Contents', []):
            output_keys_set.add(obj['Key'])
    print(f"å·²å­˜åœ¨ {len(output_keys_set)} ä¸ªè¾“å‡ºæ–‡ä»¶ï¼Œå°†è·³è¿‡å·²å¤„ç†çš„è¾“å…¥æ–‡ä»¶")

    total_start_time = time.time()
    global_valid_count = 0  # å…¨å±€æœ‰æ•ˆå›¾ç‰‡è®¡æ•°å™¨

    # åˆ†æ‰¹å¤„ç†æ–‡ä»¶
    for i in range(0, len(file_keys), BATCH_SIZE):
        batch = file_keys[i:i + BATCH_SIZE]
        print(f"\n=== å¤„ç†ç¬¬ {i//BATCH_SIZE + 1} æ‰¹æ¬¡ ({len(batch)} ä¸ªæ–‡ä»¶) ===")
        batch_valid_count = await process_batch(s3_client, batch, output_keys_set)
        global_valid_count += batch_valid_count
        print(f" å…¨å±€æœ‰æ•ˆå›¾ç‰‡æ•°é‡: {global_valid_count}")

    total_time = time.time() - total_start_time
    print(f"\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {format_time(total_time)}")
    print(f"ğŸ‰ å…¨å±€æœ‰æ•ˆå›¾ç‰‡æ€»æ•°: {global_valid_count}")

if __name__ == "__main__":
    print('******* å¼€å§‹å›¾æ–‡ç†è§£å¤„ç†æµç¨‹ ********')
    asyncio.run(main())